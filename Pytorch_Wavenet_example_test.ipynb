{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_Wavenet_example_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RNgwIpxzNVq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2z2sqDGXNVrA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wavenet  class\n",
        "Two generation functions are included. generate_slow( ) is easy to understand, but generate( ) is much faster."
      ]
    },
    {
      "metadata": {
        "id": "H0OiD8xZNVrB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Conv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
        "              dilation=1, bias=True, w_init_gain='linear', is_causal=False):\n",
        "    super(Conv, self).__init__()\n",
        "    self.is_causal = is_causal\n",
        "    self.kernel_size = kernel_size\n",
        "    self.dilation = dilation\n",
        "    \n",
        "    self.conv = nn.Conv1d(in_channels, out_channels,\n",
        "                         kernel_size=kernel_size, stride=stride,\n",
        "                         dilation=dilation, bias=bias)\n",
        "    nn.init.xavier_uniform_(self.conv.weight, \n",
        "                           gain=nn.init.calculate_gain(w_init_gain))\n",
        "    \n",
        "  def forward(self, signal):\n",
        "    if self.is_causal:\n",
        "      padding = (int((self.kernel_size - 1) * (self.dilation)), 0)\n",
        "      signal = nn.functional.pad(signal, padding)\n",
        "    return self.conv(signal)\n",
        "\n",
        "\n",
        "\n",
        "class WaveNet(nn.Module):\n",
        "    def __init__(self, mu=1,n_residue=32, n_skip= 512, dilation_depth=10, n_repeat=5, n_cond_channel=315):\n",
        "        # mu: audio quantization size\n",
        "        # n_residue: residue channels\n",
        "        # n_skip: skip channels\n",
        "        # dilation_depth & n_repeat: dilation layer setup\n",
        "        super(WaveNet, self).__init__()\n",
        "        self.dilation_depth = dilation_depth\n",
        "        dilations = self.dilations = [2**i for i in range(dilation_depth)] * n_repeat\n",
        "        self.from_input = nn.Conv1d(in_channels=mu, out_channels=n_residue, kernel_size=1)\n",
        "        self.conv_sigmoid = nn.ModuleList([Conv(in_channels=n_residue, out_channels=n_residue, \n",
        "                                                kernel_size=2, dilation=d, w_init_gain='sigmoid', is_causal=True)\n",
        "                         for d in dilations])\n",
        "        self.conv_tanh = nn.ModuleList([Conv(in_channels=n_residue, out_channels=n_residue, \n",
        "                                             kernel_size=2, dilation=d, w_init_gain='tanh', is_causal=True)\n",
        "                         for d in dilations])\n",
        "        self.skip_scale = nn.ModuleList([nn.Conv1d(in_channels=n_residue, out_channels=n_skip, kernel_size=1)\n",
        "                         for d in dilations])\n",
        "        self.residue_scale = nn.ModuleList([nn.Conv1d(in_channels=n_residue, out_channels=n_residue, kernel_size=1)\n",
        "                         for d in dilations])\n",
        "        self.conv_post_1 = nn.Conv1d(in_channels=n_skip, out_channels=n_skip, kernel_size=1)\n",
        "        self.conv_post_2 = nn.Conv1d(in_channels=n_skip, out_channels=mu, kernel_size=1)\n",
        "        \n",
        "        self.cond_conv = nn.Conv1d(in_channels=n_cond_channel, \n",
        "                                   out_channels=dilation_depth*n_repeat*n_residue, kernel_size=1)\n",
        "        self.n_layers = dilation_depth * n_repeat\n",
        "        \n",
        "    def forward(self, input, cond_input):\n",
        "        output = self.preprocess(input)\n",
        "        cond_output = self.cond_conv(cond_input.unsqueeze(0).transpose(1, 2))\n",
        "        cond_output = cond_output.view(cond_output.size(0), self.n_layers, -1, cond_output.size(2))\n",
        "\n",
        "        skip_connections = [] # save for generation purposes\n",
        "        i = 0\n",
        "        for s, t, skip_scale, residue_scale in zip(self.conv_sigmoid, self.conv_tanh, self.skip_scale, self.residue_scale):\n",
        "            output, skip = self.residue_forward(output, cond_output[:, i, :, :], s, t, skip_scale, residue_scale)\n",
        "            skip_connections.append(skip)\n",
        "            i += 1\n",
        "        # sum up skip connections\n",
        "        output = sum([s[:,:,-output.size(2):] for s in skip_connections])\n",
        "        output = self.postprocess(output)\n",
        "        return output\n",
        "    \n",
        "    def preprocess(self, input):\n",
        "        output = input.unsqueeze(1).unsqueeze(0).transpose(1,2) #self.one_hot(input).unsqueeze(0).transpose(1,2)\n",
        "        output = self.from_input(output)\n",
        "        return output\n",
        "    \n",
        "    def postprocess(self, input):\n",
        "        output = nn.functional.elu(input)\n",
        "        output = self.conv_post_1(output)\n",
        "        output = nn.functional.elu(output)\n",
        "        output = self.conv_post_2(output).squeeze(0).transpose(0,1)\n",
        "        return output\n",
        "    \n",
        "    def residue_forward(self, input, cond_act, conv_sigmoid, conv_tanh, skip_scale, residue_scale):\n",
        "        output = input\n",
        "        output_sigmoid, output_tanh = conv_sigmoid(output), conv_tanh(output)\n",
        "        output_sigmoid += cond_act\n",
        "        output_tanh += cond_act\n",
        "        output = torch.sigmoid(output_sigmoid) * torch.tanh(output_tanh)\n",
        "        skip = skip_scale(output)\n",
        "        output = residue_scale(output)\n",
        "        output = output + input[:,:,-output.size(2):]\n",
        "        return output, skip\n",
        "    \n",
        "#     def generate_slow(self, input, cond_input, n=100):\n",
        "#         res = input.data.tolist()\n",
        "#         for ii in range(n):\n",
        "#             x_prev = Variable(torch.FloatTensor(res[-sum(self.dilations)-1:]))\n",
        "#             cond_input_t = cond_input[len(res)+1-sum(self.dilations):len(res)+1, :]\n",
        "#             y = self.forward(x_prev, cond_input_t)\n",
        "# #             _, i = y.max(dim=1)\n",
        "#             res.append(y.data[-1, 0])\n",
        "#         return res\n",
        "    \n",
        "#     def generate(self, input, cond_input, n=100, temperature=None, estimate_time=False):\n",
        "#         ## prepare output_buffer\n",
        "#         output = self.preprocess(input)\n",
        "#         cond_output = self.cond_conv(cond_input.unsqueeze(0).transpose(1, 2))\n",
        "#         cond_output = cond_output.view(cond_output.size(0), self.n_layers, -1, cond_output.size(2))\n",
        "\n",
        "#         output_buffer = []\n",
        "#         i = 0\n",
        "#         for s, t, skip_scale, residue_scale, d in zip(self.conv_sigmoid, self.conv_tanh, self.skip_scale, self.residue_scale, self.dilations):\n",
        "#             output, _ = self.residue_forward(output, cond_output[:, i, :, :], s, t, skip_scale, residue_scale)\n",
        "#             sz = 1 if d==2**(self.dilation_depth-1) else d*2\n",
        "#             output_buffer.append(output[:,:,-sz-1:-1])\n",
        "#             i += 1\n",
        "#         ## generate new \n",
        "#         res = input.data.tolist()\n",
        "#         res_cond = cond_input.data.tolist()\n",
        "#         for i in range(n):\n",
        "#             output = Variable(torch.FloatTensor(res[-2:]))\n",
        "#             cond_output = Variable(torch.FloatTensor(res_cond[-2:]))\n",
        "#             output = self.preprocess(output)\n",
        "#             cond_output = self.cond_conv(cond_output.unsqueeze(0).transpose(1, 2))\n",
        "#             cond_output = cond_output.view(cond_output.size(0), self.n_layers, -1, cond_output.size(2))\n",
        "\n",
        "#             output_buffer_next = []\n",
        "#             skip_connections = [] # save for generation purposes\n",
        "#             dd = 0\n",
        "#             for s, t, skip_scale, residue_scale, b in zip(self.conv_sigmoid, self.conv_tanh, self.skip_scale, self.residue_scale, output_buffer):\n",
        "#                 output, residue = self.residue_forward(output, cond_output[:, dd, :, :], s, t, skip_scale, residue_scale)\n",
        "#                 output = torch.cat([b, output], dim=2)\n",
        "#                 skip_connections.append(residue)\n",
        "#                 dd += 1\n",
        "#                 if i%100==0:\n",
        "#                     output = output.clone()\n",
        "#                 output_buffer_next.append(output[:,:,-b.size(2):])\n",
        "#             output_buffer = output_buffer_next\n",
        "#             output = output[:,:,-1:]\n",
        "#             # sum up skip connections\n",
        "#             output = sum(skip_connections)\n",
        "#             output = self.postprocess(output)\n",
        "# #             if temperature is None:\n",
        "# #                 _, output = output.max(dim=1)\n",
        "# #             else:\n",
        "# #                 output = output.div(temperature).exp().multinomial(1).squeeze()\n",
        "#             res.append(output.data[-1, 0])\n",
        "#         return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "38FASV9DNVrS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load data and cofigure dataset\n"
      ]
    },
    {
      "metadata": {
        "id": "dRRX0kb7YCij",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "import gc\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "SubjectTaskAxis = namedtuple('SubjectTaskAxis', ['subject', 'task', 'axis'])\n",
        "\n",
        "cache_file = os.path.abspath('/content/drive/My Drive/Colab Notebooks/cached_python_all_zero_pad_data.npz')\n",
        "data = np.load(cache_file)\n",
        "subjects = data['subjects']\n",
        "tasks = data['tasks']\n",
        "axes = data['axes']\n",
        "mocap_distance = data['distance'] # \n",
        "di_distance = data['di_distance']\n",
        "dist_diff = data['dist_diff']\n",
        "length = data['length']\n",
        "di_pos = data['pos'] # filtered double integrated position\n",
        "di_pos_u = data['pos_u'] # unfiltered double integrated position\n",
        "vel = data['vel']\n",
        "vel_norm = data['vel_norm']\n",
        "acc = data['acc']\n",
        "acc_unfiltered = data['acc_unfiltered']\n",
        "direction = data['direction']\n",
        "segment_begin = data['segment_begin']\n",
        "segment_end = data['segment_end']\n",
        "mocap_pos = data['mocap_pos'][()]  # dict is stored as no shape array, access with [()]\n",
        "\n",
        "# Exclude some data points\n",
        "exclude_mask = ~((subjects == 8) | (subjects == 9))\n",
        "subjects = subjects[exclude_mask]\n",
        "tasks = tasks[exclude_mask]\n",
        "axes = axes[exclude_mask]\n",
        "mocap_distance = mocap_distance[exclude_mask]\n",
        "di_distance = di_distance[exclude_mask]\n",
        "dist_diff = dist_diff[exclude_mask]\n",
        "length = length[exclude_mask]\n",
        "di_pos = di_pos[exclude_mask, :]\n",
        "di_pos_u = di_pos_u[exclude_mask, :]\n",
        "vel = vel[exclude_mask, :]\n",
        "vel_norm = vel_norm[exclude_mask, :]\n",
        "acc = acc[exclude_mask, :]\n",
        "acc_unfiltered = acc_unfiltered[exclude_mask, :]\n",
        "direction = direction[exclude_mask]\n",
        "segment_begin = segment_begin[exclude_mask]\n",
        "segment_end = segment_end[exclude_mask]\n",
        "\n",
        "# For later convenience\n",
        "abs_distance = np.absolute(mocap_distance)\n",
        "abs_di_distance = np.absolute(di_distance)\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sbNBNBUsYNiR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MocapTrainDataset(Dataset):\n",
        "  def __init__(self, select_sub, segment_length): \n",
        "    np.random.seed(1234)\n",
        "    self.select_sub = select_sub\n",
        "    self.segment_length = segment_length\n",
        "    self.mocap_d_data = []\n",
        "    self.cond_vel_data = []\n",
        "\n",
        "    for sub in self.select_sub:\n",
        "      for t in range(1, 3):\n",
        "        for ax in range(1, 4):\n",
        "          mask = (subjects==sub) & (tasks == t) & (axes==ax)\n",
        "          self.mocap_d_data.append(mocap_distance[mask])\n",
        "          self.cond_vel_data.append(acc[mask, :])\n",
        "          \n",
        "    self.mocap_d_data = np.array(self.mocap_d_data)\n",
        "    self.cond_vel_data = np.array(self.cond_vel_data)\n",
        "\n",
        "    ind_shuffle = np.arange(len(self.mocap_d_data))\n",
        "    np.random.shuffle(ind_shuffle)\n",
        "    self.mocap_d_data = self.mocap_d_data[ind_shuffle]\n",
        "    self.cond_vel_data = self.cond_vel_data[ind_shuffle]\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    d_seq = self.mocap_d_data[index]\n",
        "    cond_vel_seq = self.cond_vel_data[index]\n",
        "    \n",
        "    if len(d_seq) >= self.segment_length:\n",
        "      max_start = len(d_seq) - self.segment_length\n",
        "      start = np.random.randint(0, max_start)\n",
        "      d_seq = d_seq[start:start+self.segment_length] \n",
        "      cond_vel_seq = cond_vel_seq[start:start+self.segment_length, :]\n",
        "    \n",
        "    d_seq = d_seq.astype(np.float32)\n",
        "    cond_vel_seq = cond_vel_seq.astype(np.float32)\n",
        "\n",
        "    return Variable(torch.from_numpy(d_seq)), Variable(torch.from_numpy(cond_vel_seq)) \n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.mocap_d_data)\n",
        "  \n",
        "  \n",
        "  \n",
        "class MocapTestDataset(Dataset):\n",
        "  def __init__(self, select_sub): \n",
        "    np.random.seed(1234)\n",
        "    self.select_sub = select_sub\n",
        "    self.mocap_d_data = []\n",
        "    self.cond_vel_data = []\n",
        "    self.di_d_data = []\n",
        "\n",
        "    for sub in self.select_sub:\n",
        "      for t in range(1, 3):\n",
        "        for ax in range(1, 4):\n",
        "          mask = (subjects==sub) & (tasks == t) & (axes==ax)\n",
        "          self.mocap_d_data.append(mocap_distance[mask])\n",
        "          self.cond_vel_data.append(acc[mask, :])\n",
        "          self.di_d_data.append(di_distance[mask])\n",
        "          \n",
        "    self.mocap_d_data = np.array(self.mocap_d_data)\n",
        "    self.cond_vel_data = np.array(self.cond_vel_data)\n",
        "    self.di_d_data = np.array(self.di_d_data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    d_seq = self.mocap_d_data[index]\n",
        "    cond_vel_seq = self.cond_vel_data[index]\n",
        "    di_d_seq = self.di_d_data[index]\n",
        "    \n",
        "    d_seq = d_seq.astype(np.float32)\n",
        "    cond_vel_seq = cond_vel_seq.astype(np.float32)\n",
        "    di_d_seq = di_d_seq.astype(np.float32)\n",
        "\n",
        "    return Variable(torch.from_numpy(di_d_seq)), Variable(torch.from_numpy(cond_vel_seq)), Variable(torch.from_numpy(d_seq))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.mocap_d_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a12LxLyFNVrZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## network training\n",
        "\n",
        "This network tries to generate the vibrating sine wave above. \n",
        "- 24 channels in residue outputs\n",
        "- 128 channels in skip outputs\n",
        "- 5 dilation layers (n_repeat=1, dilation_depth=5)"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "vUJAgsIdNVra",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_cond = len(acc[0])\n",
        "depth = 5\n",
        "net = WaveNet(mu=1,n_residue=24,n_skip=128,dilation_depth=depth,n_repeat=1, n_cond_channel=n_cond)\n",
        "batch_length = 100 \n",
        "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
        "uniq_sub = np.unique(subjects)\n",
        "train_sub = np.delete(uniq_sub, 0)\n",
        "test_sub = [uniq_sub[0]]\n",
        "trainset = MocapTrainDataset(train_sub, batch_length)\n",
        "batch_size = 8#64 #trainset.__len__()\n",
        "dataset_len = trainset.__len__()\n",
        "print('dataset lenght', dataset_len)\n",
        "print('batch size', batch_size)\n",
        "\n",
        "loss_save = []\n",
        "max_epoch = 3000\n",
        "for epoch in range(max_epoch):\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    batch_idx = [np.random.randint(0, dataset_len) for _ in range(batch_size)]\n",
        "    for idx in range(batch_size):\n",
        "        batch = trainset.__getitem__(batch_idx[idx])\n",
        "        x, y = batch \n",
        "        x_in = x[:-1]\n",
        "        y_in = y[1:, :]\n",
        "        logits = net(x_in, y_in)\n",
        "        sz = batch_length - 2**depth\n",
        "\n",
        "        loss = loss + nn.functional.mse_loss(logits[:, 0], x[1:])\n",
        "    loss = loss/batch_size\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_save.append(loss.item())\n",
        "    # monitor progress\n",
        "    if epoch%100==0:\n",
        "        print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
        "        ii = np.random.randint(0, dataset_len)\n",
        "        batch = trainset.__getitem__(ii)\n",
        "        x, y = batch \n",
        "        x_in = x[:-1]\n",
        "        y_in = y[1:, :]\n",
        "        logits = net(x_in, y_in)\n",
        "#         _, i = logits.max(dim=1)\n",
        "        plt.figure(figsize=[16,4])\n",
        "#         plt.plot(i.data.tolist())\n",
        "        sz = batch_length - 2**depth\n",
        "\n",
        "        plt.step(logits[:, 0].tolist(), 'tab:blue', label='pred')\n",
        "#         print(logits.tolist())\n",
        "        plt.step(x.tolist()[1:],'tab:orange', ms=1, label='gt')\n",
        "        plt.title('epoch {}'.format(epoch))\n",
        "        plt.legend()\n",
        "        plt.ylim(-1.22, 1.22)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Me79AJpRNVrc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## loss function"
      ]
    },
    {
      "metadata": {
        "id": "siNCAlW5NVrd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[15,4])\n",
        "plt.plot(loss_save)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('loss function')\n",
        "plt.ylim(0, 0.01)\n",
        "torch.save(net.state_dict(), '/content/drive/My Drive/Colab Notebooks/wavenet_cond_fixed')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QafghadnNVrg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## data generation"
      ]
    },
    {
      "metadata": {
        "id": "SAxxOUdXNVrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# given mocap dataset \n",
        "loaded_net = torch.load('/content/drive/My Drive/Colab Notebooks/wavenet_cond_fixed')\n",
        "net_test = WaveNet(mu=1,n_residue=24,n_skip=128,dilation_depth=depth,n_repeat=1, n_cond_channel=n_cond)\n",
        "\n",
        "net_test.load_state_dict(loaded_net)\n",
        "print(train_sub)\n",
        "print(test_sub)\n",
        "total_mae = []\n",
        "testset = MocapTestDataset(test_sub) \n",
        "for kk in range(len(testset)):\n",
        "  batch = testset.__getitem__(kk)\n",
        "\n",
        "  x, y, gt = batch\n",
        "  x_in = x[:-1]\n",
        "  y_in = y[1:, :]\n",
        "  logits = net_test(x_in, y_in)\n",
        "  plt.figure(figsize=[50,4])\n",
        "  plt.step(logits[:, 0].tolist(), 'tab:blue', label='pred')\n",
        "  plt.step(gt[1:].tolist(),'tab:orange', ms=1, label='gt')\n",
        "  plt.legend()\n",
        "  plt.ylim(-1.22, 1.22)\n",
        "  plt.ylabel('distance')\n",
        "  plt.grid()\n",
        "\n",
        "  total_mae += list(abs(np.array(logits[:, 0].tolist()) - np.array(gt[1:].tolist())))\n",
        "  \n",
        "print('total mean', np.mean(np.array(total_mae)), 'total median', np.median(total_mae))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9shW6WO28laK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# generate\n",
        "testset = MocapTestDataset(test_sub) \n",
        "\n",
        "total_gen_mae = []\n",
        "for kk in range(len(testset)):\n",
        "  batch = testset.__getitem__(kk)\n",
        "  dilation_dim = sum([2**i for i in range(depth)])\n",
        "\n",
        "  x_whole, y_whole, gt = batch \n",
        "  x_s = x_whole[:dilation_dim]\n",
        "\n",
        "  res = x_s.tolist()\n",
        "  for l in range(len(y_whole)-dilation_dim):\n",
        "    x_prev = Variable(torch.FloatTensor(res[-dilation_dim:]))\n",
        "    results = net_test(x_prev, y_whole[l+1:l+1+dilation_dim, :])\n",
        "    res.append(results[-1, 0].tolist())\n",
        "\n",
        "    \n",
        "  mask = (subjects == test_sub[0]) & (tasks == int(kk/3+1)) & (axes == int(kk % 3+1))\n",
        "  plt.figure(figsize=[21,4])\n",
        "  plt.step(res, 'tab:blue', label='pred')\n",
        "  plt.step(gt.tolist(),'tab:orange', ms=1, label='gt')\n",
        "  plt.legend()\n",
        "#   plt.ylim(-1.22, 1.22)\n",
        "  plt.ylabel('distance (m)')\n",
        "  plt.grid()\n",
        "  plt.axhline(0, color='k')\n",
        "  plt.title('sub ' + str(test_sub[0]) + ' tasks ' + str(int(kk/3+1)) + ' axes ' + str(int(kk % 3+1)))\n",
        "\n",
        "  \n",
        "  plt.figure(figsize=[21,4])\n",
        "  cum_x = np.cumsum(length[mask]) / (100 * 60)\n",
        "  plt.step(cum_x, np.cumsum(np.array(res) - np.array(gt.tolist())), 'tab:red', label='cumulatvie error')\n",
        "  plt.legend()\n",
        "  plt.ylim(-2, 2)\n",
        "  plt.ylabel('cumulatvie error (distance (m))')\n",
        "  plt.grid()\n",
        "  plt.axhline(0, color='k')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  total_gen_mae += list(abs(np.array(res) - np.array(gt.tolist())))\n",
        "  \n",
        "print('total mean', np.mean(np.array(total_gen_mae)), 'total median', np.median(total_gen_mae))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_1mSl0-nyBEb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0SAjnKyM3doz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}